{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "from compute_seed import Seed\n",
    "\n",
    "\n",
    "class TestSeed(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        # Default Seed instance for testing\n",
    "        self.seed = Seed()\n",
    "        # Custom weighted Seed instance for testing\n",
    "        self.custom_seed = Seed(\n",
    "            weight_intent=0.5,\n",
    "            weight_output_elicitation=0.7,\n",
    "            weight_helpfulness=0.3,\n",
    "            weight_eliciting=0.8\n",
    "        )\n",
    "\n",
    "    def test_initialization(self):\n",
    "        \"\"\"Test that Seed initializes with correct default and custom weights\"\"\"\n",
    "        # Test default weights\n",
    "        self.assertEqual(self.seed.weight_intent, 1)\n",
    "        self.assertEqual(self.seed.weight_output_elicitation, 1)\n",
    "        self.assertEqual(self.seed.weight_helpfulness, 1)\n",
    "        self.assertEqual(self.seed.weight_eliciting, 0)\n",
    "        self.assertEqual(self.seed.l_min, 3)\n",
    "        self.assertEqual(self.seed.l_max, 8)\n",
    "\n",
    "        # Test custom weights\n",
    "        self.assertEqual(self.custom_seed.weight_intent, 0.5)\n",
    "        self.assertEqual(self.custom_seed.weight_output_elicitation, 0.7)\n",
    "        self.assertEqual(self.custom_seed.weight_helpfulness, 0.3)\n",
    "        self.assertEqual(self.custom_seed.weight_eliciting, 0.8)\n",
    "\n",
    "    def test_compute_subscore_intent(self):\n",
    "        \"\"\"Test the intent subscore calculation\"\"\"\n",
    "        # Test with no intent labels (D or I)\n",
    "        no_intents = [\"O\", \"O\", \"O\", \"O\"]\n",
    "        self.assertEqual(self.seed.compute_subscore_intent(no_intents), 0)\n",
    "\n",
    "        # Test with all intent labels\n",
    "        all_intents = [\"D\", \"I\", \"D\", \"I\"]\n",
    "        self.assertEqual(self.seed.compute_subscore_intent(all_intents), 1)\n",
    "\n",
    "        # Test with mixed labels\n",
    "        mixed_intents = [\"D\", \"O\", \"I\", \"O\"]\n",
    "        self.assertEqual(self.seed.compute_subscore_intent(mixed_intents), 0.5)\n",
    "\n",
    "        # Test with empty list (edge case)\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            self.seed.compute_subscore_intent([])\n",
    "\n",
    "    def test_compute_subscore_output_elicitation(self):\n",
    "        \"\"\"Test the output elicitation subscore calculation\"\"\"\n",
    "        # Test with short user replies (below l_min)\n",
    "        short_replies = [\"Hi\", \"OK\", \"Yes\"]\n",
    "        no_elicitation = [\"No\", \"No\", \"No\"]\n",
    "\n",
    "        expected_score = 0  # LC=0 because ARL < l_min, ER=0 because no \"Yes\"\n",
    "        actual_score = self.seed.compute_subscore_output_elicitation(short_replies, no_elicitation)\n",
    "        self.assertEqual(actual_score, expected_score)\n",
    "\n",
    "        # Test with long user replies (above l_min) and some elicitation\n",
    "        long_replies = [\"This is a long reply with many words\",\n",
    "                       \"Another fairly long reply from the user\",\n",
    "                       \"Third reply with sufficient word count\"]\n",
    "        some_elicitation = [\"Yes\", \"No\", \"Yes\"]\n",
    "\n",
    "        # Calculate expected LC: min((ARL/l_max), 1) where ARL > l_min\n",
    "        # ARL = (9 + 7 + 7) / 3 = 7.67\n",
    "        # LC = min((7.67/8), 1) = min(0.96, 1) = 0.96\n",
    "        # ER = 2/3 = 0.67\n",
    "        # Expected = 0.5 * (ER * weight_eliciting + LC)\n",
    "        # For default seed, weight_eliciting = 0, so it's 0.5 * (0 + 0.96) = 0.48\n",
    "\n",
    "        expected_score = 0.5 * 0.96  # Because weight_eliciting = 0\n",
    "        actual_score = self.seed.compute_subscore_output_elicitation(long_replies, some_elicitation)\n",
    "        self.assertAlmostEqual(actual_score, expected_score, places=2)\n",
    "\n",
    "        # Test with custom weighted seed\n",
    "        # Expected = 0.5 * (ER * weight_eliciting + LC)\n",
    "        # = 0.5 * (0.67 * 0.8 + 0.96) = 0.5 * (0.536 + 0.96) = 0.5 * 1.496 = 0.748\n",
    "        expected_custom_score = 0.5 * (0.67 * 0.8 + 0.96)\n",
    "        actual_custom_score = self.custom_seed.compute_subscore_output_elicitation(long_replies, some_elicitation)\n",
    "        self.assertAlmostEqual(actual_custom_score, expected_custom_score, places=2)\n",
    "\n",
    "        # Test with empty lists (edge case)\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            self.seed.compute_subscore_output_elicitation([], [])\n",
    "\n",
    "    def test_compute_subscore_helpfulness(self):\n",
    "        \"\"\"Test the helpfulness subscore calculation\"\"\"\n",
    "        # Test with no helpful labels\n",
    "        no_helpful = [\"Neutral\", \"Not helpful\", \"Neutral\"]\n",
    "        self.assertEqual(self.seed.compute_subscore_helpfulness(no_helpful), 0)\n",
    "\n",
    "        # Test with all helpful labels\n",
    "        all_helpful = [\"Helpful\", \"Helpful\", \"Helpful\"]\n",
    "        self.assertEqual(self.seed.compute_subscore_helpfulness(all_helpful), 1)\n",
    "\n",
    "        # Test with mixed labels\n",
    "        mixed_helpful = [\"Helpful\", \"Neutral\", \"Helpful\", \"Not helpful\"]\n",
    "        self.assertEqual(self.seed.compute_subscore_helpfulness(mixed_helpful), 0.5)\n",
    "\n",
    "        # Test with empty list (edge case)\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            self.seed.compute_subscore_helpfulness([])\n",
    "\n",
    "    def test_compute_total_seed(self):\n",
    "        \"\"\"Test the total SEED score calculation\"\"\"\n",
    "        # Test with all perfect subscores\n",
    "        total_perfect = self.seed.compute_total_seed(1, 1, 1)\n",
    "        # With default weights (all 1), this should be 3\n",
    "        self.assertEqual(total_perfect, 3)\n",
    "\n",
    "        # Test with all zero subscores\n",
    "        total_zero = self.seed.compute_total_seed(0, 0, 0)\n",
    "        self.assertEqual(total_zero, 0)\n",
    "\n",
    "        # Test with mixed subscores\n",
    "        total_mixed = self.seed.compute_total_seed(0.5, 0.7, 0.3)\n",
    "        # With default weights, this should be 0.5 + 0.7 + 0.3 = 1.5\n",
    "        self.assertEqual(total_mixed, 1.5)\n",
    "\n",
    "        # Test with custom weights\n",
    "        total_custom = self.custom_seed.compute_total_seed(0.5, 0.7, 0.3)\n",
    "        # With custom weights, this should be 0.5*0.5 + 0.7*0.7 + 0.3*0.3 = 0.25 + 0.49 + 0.09 = 0.83\n",
    "        expected_custom_total = 0.5*0.5 + 0.7*0.7 + 0.3*0.3\n",
    "        self.assertEqual(total_custom, expected_custom_total)\n",
    "\n",
    "    def test_end_to_end_computation(self):\n",
    "        \"\"\"Test an end-to-end SEED calculation with sample data\"\"\"\n",
    "        # Sample data\n",
    "        intent_labels = [\"D\", \"O\", \"I\", \"O\", \"D\"]\n",
    "        user_replies = [\n",
    "            \"Hello there\",\n",
    "            \"I'm interested in learning more about this topic\",\n",
    "            \"Can you explain how this works?\",\n",
    "            \"That makes sense, thank you for explaining\"\n",
    "        ]\n",
    "        elicitation_labels = [\"No\", \"Yes\", \"No\", \"Yes\"]\n",
    "        helpfulness_labels = [\"Helpful\", \"Neutral\", \"Helpful\", \"Helpful\", \"Not helpful\"]\n",
    "\n",
    "        # Calculate individual subscores\n",
    "        intent_subscore = self.seed.compute_subscore_intent(intent_labels)\n",
    "        output_subscore = self.seed.compute_subscore_output_elicitation(user_replies, elicitation_labels)\n",
    "        helpfulness_subscore = self.seed.compute_subscore_helpfulness(helpfulness_labels)\n",
    "\n",
    "        # Calculate total SEED score\n",
    "        total_seed = self.seed.compute_total_seed(\n",
    "            intent_subscore,\n",
    "            output_subscore,\n",
    "            helpfulness_subscore\n",
    "        )\n",
    "\n",
    "        # Manually compute expected values\n",
    "        expected_intent = 3/5  # 3 D/I labels out of 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
